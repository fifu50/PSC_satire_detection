{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Common import\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "import asyncio\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "#Plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "#datasets\n",
    "from standardize_datasets import get_merge_dataset\n",
    "from standardize_datasets import clean_unicode\n",
    "from standardize_datasets import standardize_without_theme\n",
    "from standardize_datasets import remove_punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports for NLTK\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tag import pos_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/thomasloux/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/thomasloux/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/thomasloux/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/thomasloux/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#download nltk data\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports for Spacy\n",
    "import spacy\n",
    "from spacy.lang.en.stop_words import STOP_WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = get_merge_dataset()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing functions to preprocess using nltk or spacy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def remove_stopwords_and_lemmatize_nltk(text):\n",
    "    # Doesn't take into account the POS\n",
    "    word_tokens = word_tokenize(text, language='english')\n",
    "    lemmatized_sentence = [lemmatizer.lemmatize(w) for w in word_tokens if not w in stop_words] \n",
    "    return lemmatized_sentence"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def remove_stopwords_and_lemmatize_spacy_fast(text):\n",
    "    doc = nlp(text, disable=['parser', 'ner'])\n",
    "    lemmatized_sentence = [w.lemma_ for w in doc if w.text not in STOP_WORDS ]\n",
    "    return lemmatized_sentence\n",
    "\n",
    "def remove_stopwords_and_lemmatize_spacy(text):\n",
    "    doc = nlp(text, disable=['parser', 'ner']) \n",
    "    lemmatized_sentence = [w.lemma_ for w in doc if w.text not in STOP_WORDS ]\n",
    "    return lemmatized_sentence\n",
    "\n",
    "def remove_stopwords_and_lemmatize_spacy_batch(batch):\n",
    "    docs = nlp.pipe(batch, batch_size=1000, disable=['parser', 'ner'])\n",
    "    lemmatized_sentence = [[w.lemma_ for w in doc if w.text not in STOP_WORDS] for doc in docs]\n",
    "    return lemmatized_sentence\n",
    "\n",
    "def remove_stopwords_and_lemmatize_spacy_multiprocessed(batch):\n",
    "    docs = nlp.pipe(batch, batch_size=1000, n_process=4,disable=['parser', 'ner'])\n",
    "    lemmatized_sentence = [[w.lemma_ for w in doc if w.text not in STOP_WORDS] for doc in docs]\n",
    "    return lemmatized_sentence"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparing lemmatized sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WASHINGTON—Saying  the sender’s contributions were appreciated but ultimately  self-defeating, Capitol Police told reporters Wednesday that a  thoughtful letter on how to improve the legislative process was undercut  by the poison powder included in the envelope. “Although the anonymous  assailant’s note had some helpful feedback on how to break through  partisan gridlock, the deadly quantity of ricin spread on those pages  also makes you wonder how much he really believes in improving the  democratic process,” said Capitol Police spokesperson Jermaine Williams,  rushing to note that the letter itself was well thought out, thoroughly  researched, and expressed important points on places where Republicans  and Democrats might come together to advance bills on pharmaceutical  costs and energy permitting reform. “What’s especially nice is how much  constructive feedback it contains. Sometimes people can get really nasty  about politics. But, aside from a few typos, blood-stains, and the  toxic powder, this really seems to be coming from someone who cares  about these issues and wants to see the country do better.” Williams  added that the biggest shame was that all these interesting points would unfortunately be overshadowed by the three congressional  aides sent to the hospital.\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "example = df['article'][0]\n",
    "print(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_tag(word_tokenize(example, language='english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'get_wordnet_pos' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mprint\u001b[39m(remove_stopwords_and_lemmatize_nltk(example))\n\u001b[1;32m      2\u001b[0m \u001b[39mprint\u001b[39m(remove_stopwords_and_lemmatize_spacy(example))\n",
      "Cell \u001b[0;32mIn[18], line 12\u001b[0m, in \u001b[0;36mremove_stopwords_and_lemmatize_nltk\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m     10\u001b[0m wordnet_tagged \u001b[39m=\u001b[39m \u001b[39mmap\u001b[39m(\u001b[39mlambda\u001b[39;00m x: (x[\u001b[39m0\u001b[39m], get_wordnet_pos(x[\u001b[39m1\u001b[39m])), pos_tagged)\n\u001b[1;32m     11\u001b[0m \u001b[39m# Use the wordnet tag to lemmatize the word\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m lemmatized_sentence \u001b[39m=\u001b[39m [lemmatizer\u001b[39m.\u001b[39mlemmatize(word, tag) \u001b[39mfor\u001b[39;00m word, tag \u001b[39min\u001b[39;00m wordnet_tagged \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m word \u001b[39min\u001b[39;00m stop_words]\n\u001b[1;32m     13\u001b[0m \u001b[39mreturn\u001b[39;00m lemmatized_sentence\n",
      "Cell \u001b[0;32mIn[18], line 12\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     10\u001b[0m wordnet_tagged \u001b[39m=\u001b[39m \u001b[39mmap\u001b[39m(\u001b[39mlambda\u001b[39;00m x: (x[\u001b[39m0\u001b[39m], get_wordnet_pos(x[\u001b[39m1\u001b[39m])), pos_tagged)\n\u001b[1;32m     11\u001b[0m \u001b[39m# Use the wordnet tag to lemmatize the word\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m lemmatized_sentence \u001b[39m=\u001b[39m [lemmatizer\u001b[39m.\u001b[39mlemmatize(word, tag) \u001b[39mfor\u001b[39;00m word, tag \u001b[39min\u001b[39;00m wordnet_tagged \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m word \u001b[39min\u001b[39;00m stop_words]\n\u001b[1;32m     13\u001b[0m \u001b[39mreturn\u001b[39;00m lemmatized_sentence\n",
      "Cell \u001b[0;32mIn[18], line 10\u001b[0m, in \u001b[0;36mremove_stopwords_and_lemmatize_nltk.<locals>.<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m      8\u001b[0m pos_tagged \u001b[39m=\u001b[39m pos_tag(word_tokens)\n\u001b[1;32m      9\u001b[0m \u001b[39m# Use the POS tag to get the wordnet tag\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m wordnet_tagged \u001b[39m=\u001b[39m \u001b[39mmap\u001b[39m(\u001b[39mlambda\u001b[39;00m x: (x[\u001b[39m0\u001b[39m], get_wordnet_pos(x[\u001b[39m1\u001b[39m])), pos_tagged)\n\u001b[1;32m     11\u001b[0m \u001b[39m# Use the wordnet tag to lemmatize the word\u001b[39;00m\n\u001b[1;32m     12\u001b[0m lemmatized_sentence \u001b[39m=\u001b[39m [lemmatizer\u001b[39m.\u001b[39mlemmatize(word, tag) \u001b[39mfor\u001b[39;00m word, tag \u001b[39min\u001b[39;00m wordnet_tagged \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m word \u001b[39min\u001b[39;00m stop_words]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'get_wordnet_pos' is not defined"
     ]
    }
   ],
   "source": [
    "print(remove_stopwords_and_lemmatize_nltk(example))\n",
    "print(remove_stopwords_and_lemmatize_spacy(example))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PHOENIX—Catering to a large and valuable segment of customers who have misguided visions of what city living will be like, truck rental company U-Haul announced Wednesday that it is now offering a discount to customers who will just end up moving back home in 18 months after failing to make it in a major metropolitan area. “Beginning today, we’re taking 30 percent off our mileage rate for those who pack up their belongings, head off to a large city, give their dreams a feeble shot, and then come crawling right back to the safety and security of home within a year and a half,” said U-Haul spokesman Christine Shipley, adding that the deal would apply to all 10-foot trucks designated for moving into studios and one-bedroom apartments, most of which customers will leave behind before their lease is even up. “We are also including free furniture blankets, bubble wrap, and boxes for the items that may not even be fully unpacked during the short time it takes for our customers to be spit right back out of the big city. If you’re making the big move that you truly are not prepared for at this point in your life, you can’t afford to pass up this deal.” The new offer follows U-Haul’s widely popular half-off discount for those who will take three weeks to deeply regret moving in with their significant other.\n",
      "\n",
      "\n",
      "['PHOENIX—Catering', 'large', 'valuable', 'segment', 'customer', 'misguided', 'vision', 'city', 'living', 'like', ',', 'truck', 'rental', 'company', 'U-Haul', 'announced', 'Wednesday', 'offering', 'discount', 'customer', 'end', 'moving', 'back', 'home', '18', 'month', 'failing', 'make', 'major', 'metropolitan', 'area', '.', '“', 'Beginning', 'today', ',', '’', 'taking', '30', 'percent', 'mileage', 'rate', 'pack', 'belonging', ',', 'head', 'large', 'city', ',', 'give', 'dream', 'feeble', 'shot', ',', 'come', 'crawling', 'right', 'back', 'safety', 'security', 'home', 'within', 'year', 'half', ',', '”', 'said', 'U-Haul', 'spokesman', 'Christine', 'Shipley', ',', 'adding', 'deal', 'would', 'apply', '10-foot', 'truck', 'designated', 'moving', 'studio', 'one-bedroom', 'apartment', ',', 'customer', 'leave', 'behind', 'lease', 'even', '.', '“', 'We', 'also', 'including', 'free', 'furniture', 'blanket', ',', 'bubble', 'wrap', ',', 'box', 'item', 'may', 'even', 'fully', 'unpacked', 'short', 'time', 'take', 'customer', 'spit', 'right', 'back', 'big', 'city', '.', 'If', '’', 'making', 'big', 'move', 'truly', 'prepared', 'point', 'life', ',', '’', 'afford', 'pas', 'deal.', '”', 'The', 'new', 'offer', 'follows', 'U-Haul', '’', 'widely', 'popular', 'half-off', 'discount', 'take', 'three', 'week', 'deeply', 'regret', 'moving', 'significant', '.']\n",
      "['PHOENIX', '—', 'cater', 'large', 'valuable', 'segment', 'customer', 'misguide', 'vision', 'city', 'living', 'like', ',', 'truck', 'rental', 'company', 'u', '-', 'Haul', 'announce', 'Wednesday', 'offer', 'discount', 'customer', 'end', 'move', 'home', '18', 'month', 'fail', 'major', 'metropolitan', 'area', '.', '\"', 'begin', 'today', ',', 'take', '30', 'percent', 'mileage', 'rate', 'pack', 'belonging', ',', 'head', 'large', 'city', ',', 'dream', 'feeble', 'shot', ',', 'come', 'crawl', 'right', 'safety', 'security', 'home', 'year', 'half', ',', '\"', 'say', 'U', '-', 'Haul', 'spokesman', 'Christine', 'Shipley', ',', 'add', 'deal', 'apply', '10', '-', 'foot', 'truck', 'designate', 'move', 'studio', '-', 'bedroom', 'apartment', ',', 'customer', 'leave', 'lease', '.', '\"', 'we', 'include', 'free', 'furniture', 'blanket', ',', 'bubble', 'wrap', ',', 'box', 'item', 'fully', 'unpacked', 'short', 'time', 'take', 'customer', 'spit', 'right', 'big', 'city', '.', 'if', 'make', 'big', 'truly', 'prepare', 'point', 'life', ',', 'afford', 'pass', 'deal', '.', '\"', 'the', 'new', 'offer', 'follow', 'U', '-', 'Haul', 'widely', 'popular', 'half', '-', 'discount', 'week', 'deeply', 'regret', 'move', 'significant', '.', '\\n\\n']\n"
     ]
    }
   ],
   "source": [
    "print(example)\n",
    "print(remove_stopwords_and_lemmatize_nltk(example))\n",
    "print(remove_stopwords_and_lemmatize_spacy(example))\n",
    "\n",
    "#Performance of spacy seems to be better than nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, o = standardize_without_theme()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'HOLLYWOOD, CA—The new Jerry Bruckheimer comedy Kangaroo Jack has successfully tapped into America&#39;s longstanding love affair with rapping kangaroos, taking in a box-office-best $17.7 million in its opening weekend. &quot;From Krazy Legs Kangol in the early &#39;80s to such New School acts as Pouch Gangstas and Tha Mar$upials, kangaroos have always been at the forefront of the rap scene,&quot; media analyst Glen Coffey said. &quot;But not until now has anyone had the vision to exploit this trend in a full-length feature film.&quot; Warner Bros. has already confirmed plans for a sequel, Koala Bob, featuring a computer-generated beat-boxing koala who steals $50 million in gold bullion… and he&#39;s not giving it back.\\n\\n'"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "o['article'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "o['article'] = o['article'].apply(clean_unicode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "o['article'] = o['article'].apply(remove_punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"HOLLYWOOD  CA The new Jerry Bruckheimer comedy Kangaroo Jack has successfully tapped into America's longstanding love affair with rapping kangaroos  taking in a box office best  17 7 million in its opening weekend   From Krazy Legs Kangol in the early '80s to such New School acts as Pouch Gangstas and Tha Mar upials  kangaroos have always been at the forefront of the rap scene   media analyst Glen Coffey said   But not until now has anyone had the vision to exploit this trend in a full length feature film   Warner Bros  has already confirmed plans for a sequel  Koala Bob  featuring a computer generated beat boxing koala who steals  50 million in gold bullion  and he's not giving it back \""
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "o['article'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"HOLLYWOOD   CA the new Jerry Bruckheimer comedy Kangaroo Jack successfully tap America ' s longstanding love affair rap kangaroo   take box office good   17 7 million opening weekend    from Krazy Legs Kangol early ' 80 New School act Pouch Gangstas Tha Mar upial   kangaroo forefront rap scene    medium analyst Glen Coffey say    but vision exploit trend length feature film    Warner Bros   confirm plan sequel   Koala Bob   feature computer generate beat box koala steal   50 million gold bullion   ' s give\""
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join(remove_stopwords_and_lemmatize_spacy(o['article'][0]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test performance for removing stopwords and lemmatizing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "817 µs ± 123 µs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit remove_stopwords_and_lemmatize_nltk(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44.7 ms ± 178 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit remove_stopwords_and_lemmatize_spacy(example)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLTK seems way faster but it doesn't provide the same results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21.6 ms ± 384 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit remove_stopwords_and_lemmatize_spacy_fast(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21.9 ms ± 560 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit remove_stopwords_and_lemmatize_spacy_batch([example])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1min 32s ± 1.36 s per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit remove_stopwords_and_lemmatize_spacy_batch(df['article'][0:2000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.9 s ± 120 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit df['article'][0:100].apply(remove_stopwords_and_lemmatize_spacy_fast)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40.1 s ± 1.07 s per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit remove_stopwords_and_lemmatize_spacy_multiprocessed(df['article'][0:2000])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for t1, t2, t3 in zip(word_tokenize(example), example.split(\" \"), nlp(example)):\n",
    "#    print(t1,\"---------\" , t2, '--------', t3.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_short = df[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/db/g2vsbpsn1vg4l35x0xjtrjhc0000gn/T/ipykernel_8301/103940869.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_short['article'] = df_short['article'].apply(remove_stopwords_and_lemmatize_spacy)\n"
     ]
    }
   ],
   "source": [
    "df_short['article'] = df_short['article'].apply(remove_stopwords_and_lemmatize_spacy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26116\n"
     ]
    }
   ],
   "source": [
    "#Using Tokenizer from tf.keras\n",
    "tokenizer = Tokenizer(num_words=1000, oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(df_short['article'])\n",
    "word_index = tokenizer.word_index\n",
    "print(len(word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('<OOV>', 1), (',', 2), ('.', 3), ('\"', 4), ('-', 5), ('i', 6), ('say', 7), ('the', 8), ('year', 9), ('–', 10)]\n"
     ]
    }
   ],
   "source": [
    "print(list(word_index.items())[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1],\n",
       " [61],\n",
       " [1],\n",
       " [297],\n",
       " [1],\n",
       " [1],\n",
       " [280],\n",
       " [1],\n",
       " [1],\n",
       " [299],\n",
       " [1],\n",
       " [22],\n",
       " [],\n",
       " [1],\n",
       " [1],\n",
       " [42],\n",
       " [1],\n",
       " [],\n",
       " [1],\n",
       " [277],\n",
       " [300],\n",
       " [183],\n",
       " [1],\n",
       " [280],\n",
       " [77],\n",
       " [341],\n",
       " [90],\n",
       " [725],\n",
       " [65],\n",
       " [294],\n",
       " [226],\n",
       " [1],\n",
       " [434],\n",
       " [],\n",
       " [],\n",
       " [198],\n",
       " [240],\n",
       " [],\n",
       " [41],\n",
       " [417],\n",
       " [1],\n",
       " [1],\n",
       " [166],\n",
       " [1],\n",
       " [1],\n",
       " [],\n",
       " [145],\n",
       " [297],\n",
       " [299],\n",
       " [],\n",
       " [1],\n",
       " [1],\n",
       " [583],\n",
       " [],\n",
       " [26],\n",
       " [1],\n",
       " [59],\n",
       " [984],\n",
       " [394],\n",
       " [90],\n",
       " [9],\n",
       " [184],\n",
       " [],\n",
       " [],\n",
       " [7],\n",
       " [1],\n",
       " [],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [],\n",
       " [66],\n",
       " [72],\n",
       " [1],\n",
       " [133],\n",
       " [],\n",
       " [813],\n",
       " [1],\n",
       " [1],\n",
       " [341],\n",
       " [1],\n",
       " [],\n",
       " [1],\n",
       " [1],\n",
       " [],\n",
       " [280],\n",
       " [53],\n",
       " [1],\n",
       " [],\n",
       " [],\n",
       " [30],\n",
       " [64],\n",
       " [292],\n",
       " [1],\n",
       " [1],\n",
       " [],\n",
       " [1],\n",
       " [1],\n",
       " [],\n",
       " [774],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [445],\n",
       " [14],\n",
       " [41],\n",
       " [280],\n",
       " [1],\n",
       " [59],\n",
       " [74],\n",
       " [299],\n",
       " [],\n",
       " [78],\n",
       " [105],\n",
       " [74],\n",
       " [1],\n",
       " [887],\n",
       " [68],\n",
       " [122],\n",
       " [],\n",
       " [1],\n",
       " [315],\n",
       " [72],\n",
       " [],\n",
       " [],\n",
       " [8],\n",
       " [24],\n",
       " [183],\n",
       " [140],\n",
       " [1],\n",
       " [],\n",
       " [1],\n",
       " [1],\n",
       " [1],\n",
       " [184],\n",
       " [],\n",
       " [1],\n",
       " [38],\n",
       " [1],\n",
       " [1],\n",
       " [341],\n",
       " [495],\n",
       " [],\n",
       " []]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.texts_to_sequences(df_short['article'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useless as it doesn't speed up the process\n",
    "\n",
    "async def lemmatize_token(token):\n",
    "    if token.text not in STOP_WORDS:\n",
    "        return token.lemma_\n",
    "    else:\n",
    "        return \"\"\n",
    "\n",
    "async def remove_stopwords_and_lemmatize_async(text):\n",
    "    doc = nlp(text)\n",
    "    tasks = [lemmatize_token(token) for token in doc]\n",
    "    lemmatized_tokens = await asyncio.gather(*tasks)\n",
    "    return lemmatized_tokens"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
