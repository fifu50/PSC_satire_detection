{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scrap the website the onion\n",
    "#https://www.theonion.com/\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import json\n",
    "import datetime\n",
    "from tqdm import tqdm\n",
    "import sys\n",
    "import os\n",
    "\n",
    "#async \n",
    "import aiohttp\n",
    "import asyncio\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fisrt find all article links using the sitemap\n",
    "\n",
    "url = \"https://www.theonion.com/sitemap/\"\n",
    "months = [\"january\",\"february\",\"march\",\"april\",\"may\",\"june\",\"july\",\"august\",\"september\",\"october\",\"november\",\"december\"]\n",
    "\n",
    "#if the folder the_onion does not exist, create it\n",
    "if not os.path.exists(\"the_onion\"):\n",
    "    os.makedirs(\"the_onion\")\n",
    "\n",
    "\n",
    "for year in range(2003,2024):\n",
    "    liste = []\n",
    "    for month in months:\n",
    "        url_request = url + str(year) + \"/\" + month\n",
    "        page = requests.get(url_request, headers={'User-Agent': 'Mozilla/5.0'})\n",
    "        soup = BeautifulSoup(page.content, 'html.parser')\n",
    "        links = soup.find_all('h4')\n",
    "        for link in links:\n",
    "            liste.append(link.find('a')['href'])\n",
    "    df = pd.DataFrame(liste)\n",
    "    df.to_csv(\"the_onion/links_\" + str(year) + \".csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scrap the articles using the links in the dataframe\n",
    "\n",
    "def scrap_the_onion_asyn(url):\n",
    "    page = requests.get(url, headers={'User-Agent': 'Mozilla/5.0'})\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "    content = soup.find('main').find(\"script\").get_text()\n",
    "    content = json.loads(content) #will output a dictionary in string format, so I use json.loads to convert it to a dictionary\n",
    "\n",
    "\n",
    "    #Temporary\n",
    "\n",
    "    #print(soup.prettify())\n",
    "    #Temporary\n",
    "\n",
    "    #get theme\n",
    "    theme = soup.find_all('div', {'class' : \"sc-fek4t4-1 fKyolL\"})[0].get_text()\n",
    "\n",
    "    #convert date to datetime format\n",
    "    #remove hours\n",
    "    content[\"datePublished\"] = content[\"datePublished\"].split(\"T\")[0]\n",
    "    date = datetime.datetime.strptime(content[\"datePublished\"], '%Y-%m-%d')\n",
    "\n",
    "    return content[\"headline\"], date, theme, content[\"articleBody\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df[\"headline\"], df[\"date\"], df[\"article\"] = zip(*df[\"link\"].apply(scrap_the_onion))\n",
    "#fast way to scrap all the articles but it will take a long time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read each csv file and scrap the articles\n",
    "\n",
    "#store in a dico all the erros that occured\n",
    "errors = {}\n",
    "\n",
    "for year in range(2003,2024):\n",
    "    print(\"scraping year \" + str(year) + \"...\")\n",
    "    df = pd.read_csv(\"the_onion/links_\" + str(year) + \".csv\", header=None)\n",
    "    df.columns = [\"link\"]\n",
    "    df[\"headline\"] = \"\"\n",
    "    df[\"date\"] = \"\"\n",
    "    df[\"article\"] = \"\"\n",
    "    df[\"theme\"] = \"\"\n",
    "    for i in tqdm(range(len(df))):\n",
    "        try:\n",
    "            df[\"headline\"][i], df[\"date\"][i], df[\"theme\"][i], df[\"article\"][i] = scrap_the_onion(df[\"link\"][i])\n",
    "        except Exception as e:\n",
    "            df[\"headline\"][i], df[\"date\"][i], df[\"theme\"][i], df[\"article\"][i] = np.nan, np.nan, np.nan, np.nan\n",
    "            errors[(year, i)] = e\n",
    "            #print(e.__class__.__name__ + \": \" + str(e))\n",
    "\n",
    "    df.to_csv(\"the_onion/articles_\" + str(year) + \".csv\", index=False)\n",
    "    print(\"year \" + str(year) + \" done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#On est à l'année 2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def fetch(session, url):\n",
    "    try:\n",
    "        async with session.get(url, headers={'User-Agent': 'Mozilla/5.0'}) as response:\n",
    "            return await response.text()\n",
    "    except Exception as e:\n",
    "        return False\n",
    "async def scrap_the_onion_asyn(session, url):\n",
    "    html = await fetch(session, url)\n",
    "    if html == False:\n",
    "        return np.nan, np.nan, np.nan, np.nan\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "    try:\n",
    "        content = soup.find('main').find(\"script\").get_text()\n",
    "        content = json.loads(content)\n",
    "        theme = soup.find_all('div', {'class': \"sc-fek4t4-1 fKyolL\"})\n",
    "        if len(theme) == 0:\n",
    "            theme = np.nan\n",
    "        else:\n",
    "            theme = theme[0].get_text()\n",
    "        content[\"datePublished\"] = content[\"datePublished\"].split(\"T\")[0]\n",
    "        date = datetime.datetime.strptime(content[\"datePublished\"], '%Y-%m-%d')\n",
    "\n",
    "        return content[\"headline\"], date, theme, content[\"articleBody\"]\n",
    "    except:\n",
    "        return np.nan, np.nan, np.nan, np.nan\n",
    "\n",
    "\n",
    "\n",
    "async def main_search(year):\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        df = pd.read_csv(f\"the_onion/links_{year}.csv\", header=None)\n",
    "        urls = df[0].tolist()\n",
    "        tasks = []\n",
    "        for url in urls:\n",
    "            tasks.append(scrap_the_onion_asyn(session, url))\n",
    "        results = await asyncio.gather(*tasks)\n",
    "        df[\"headline\"], df[\"date\"], df[\"theme\"], df[\"article\"] = zip(*results)\n",
    "        df.to_csv(f\"the_onion/articles_{year}.csv\", index=False)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    if not os.path.exists(\"the_onion\"):\n",
    "        os.makedirs(\"the_onion\")\n",
    "    loop = asyncio.get_event_loop()\n",
    "    for year in range(2003, 2024):\n",
    "        loop.run_until_complete(main_search(year))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Note : The previous code ends in 14min20 while the sync version needed around 15hours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
